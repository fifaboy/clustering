\documentclass[twoside, 11pt]{article}

\usepackage{jmlr2e}
\usepackage{subfiles}
\usepackage{amsmath, amsfonts, enumerate, color, url}

%\usepackage{amsthm, amssymb, amsfonts, amsmath, geometry, enumerate}
%\geometry{left=1in,right=1in,top=.5in,bottom=1in,headheight=4pt,headsep=0.3in}


\newcommand{\x}{\mathbf{x}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\z}{\mathbf{z}}
\renewcommand{\c}{\mathbf{c}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\I}{\mathcal{I}}
\renewcommand{\S}{\mathcal{S}}

\title{Seeding In \texttt{k-means} Clustering}
\author{
	\name Masum Billal 
	\email billalmasum93@gmail.com\\
	\name Md. Walid Amin Khan
	\email walid.sec3.034@gmail.com\\
	\name Farhad Alam Bhuiyan
	\email farhad.alam@shohoz.com\\
	\name Khandaker Amitabh Nobel
	%\email nobel.khandaker@shohoz.com\\
%	\addr Data Science Department\\
	Shohoz\\
	Bangladesh
}
\editor{Not assigned}
\begin{document}
	\bibliographystyle{plain}
	\maketitle
		\begin{abstract}%
			\texttt{k-means} is a popular clustering algorithm that aims to reduce the sum of minimum squared distances from data points to the centers. Now a days most of the times seeded centers are used instead of choosing all uniformly at random. This paper will serve for multiple purposes. We express our concerns about the results presented in \texttt{k-means++} paper.We will discuss various types of seeding methods and compare their performances. Therefore, this paper also serves as an updated survey of \texttt{k-means} initial center seeding methods. Our conclusion is that, contrary to popular belief, usually  other initialization methods perform better than \texttt{k-means++}. We also prove a theorem on upper bound of inertia for any \texttt{k-means} method.
		\end{abstract}
	\section{Introduction}
	Widely regarded as the most popular clustering techniques, \texttt{k-means} remains a humble interesting topic in machine learning as well as computational geometry. Roughly the problem is: given a set of points $\X$ in $\mathbb{R}^d$. Find a set of centers $\mathcal{C}$ such that the function \textit{inertia}
		\begin{align*}
			\mathcal{I} & = \sum_{\x\in\X}\min_{\c\in\C}(\|\c-\x\|^2)
		\end{align*}
	is minimum where $\|\cdot\|$ is the $L_2$ norm\footnote{$\|c-x\|$ or $L_2$ norm of $\c-\x$ is the distance between the center $\c$ and point $\x$ or the magnitude of the vector $\c-\x$.}.
	
	Default \texttt{k-means} algorithm starts with random centers and then converge based on minimum distances of the centers from the data points. New centers are calculated based on the centroid. This is known as Lloyd's algorithm \citep{lloyd}. We repeat this process until no more change is possible. \cite{ostrovsky} and \cite{kmeans++} take it one step further by choosing the initial centers with a probability. And today, \texttt{k-means++} is without a doubt the most popular clustering technique. We intend to introduce other ways of initialization and compare their performances in terms of inertia, convergence speed and CPU time taken.
	
	First, we will discuss a sensitive issue regarding \texttt{k-means++} algorithm. Then we discuss the initialization methods used to compare the performance of clustering. We also prove a theorem that provides an upper bound for the inertia when \texttt{k-means} is done using seeded centers. Finally, we show experimental results for both our argument regarding \texttt{k-means++} and performance of initialization methods. For experimental results, we assumed that an algorithm converges only when the centers do not change anymore, that is, the algorithm absolutely stops changing centers altogether. There are some available techniques of stopping \texttt{k-means} algorithms but we chose not to use any of them as they might produce unreliable results.
	\section{Related Work}
	There has been multiple surveys on \texttt{k-means} in the literature. Probably the most relevant work in this regard is done by \cite{celebi}. However, most of the algorithms used in that paper are not used practically very much. It was also noted by the authors themselves that \texttt{k-means++} and its greedy version work better than most. They also mention that probabilistic algorithms perform better than deterministic ones. Moreover, the not so popular algorithm \citep{ostrovsky} (\texttt{ORSS}) was not considered in their experiments. There is no mention of Ostrovsky's algorithm in their paper whatsoever. Experimental results do not agree with their decision of not including \texttt{ORSS} in their survey. Of course there are other algorithms such as \cite{forgy}, etc but as we mentioned, most of them are not used practically as much. Therefore, we focused our attention only on the ones that matter the most.
	
	We would like to point out that to our knowledge no surveys were done after removing linear dependency prior to running the experiments. This is a very important step if we are to get a meaningful clustering out of \texttt{k-means} algorithm. We can achieve this using some known algorithm such as \textbf{principle component analysis} or \textbf{singular value decomposition}. We opted for PCA in this paper. PCA decomposes the existing data points into orthogonal\footnote{It is well known that orthogonal vectors are linearly independent.} ones. If we use PCA (principal component analysis) before running a clustering algorithm, we can redefine the variables into linearly independent ones. For our experiment, we have used PCA on every data set before running cluster algorithm. However, we did not reduce dimensions in order to preserve the originality of the data set. We only used PCA for removing linear dependency among variables. We strongly believe this strengthens our result over other available results.
	\section{How Good Is \texttt{k-means++}?}\label{sec:kmeans++vsorss}
	Given how simple \texttt{k-means++} is, it seems very justified that it is the most popular algorithm for \texttt{k-means}. Even in the very well known scientific python package \texttt{scikit-learn} \citep{sklearn}, \texttt{k-means++} initialization is supported by default. And it is no surprise considering the theoretical and experimental results are presented in \citep{kmeans++}. For example, \cite{kmeans++} claim \texttt{k-means++} is $\log{k}$ competitive which is a very lucrative result without any doubt. Moreover, they show experimental results that \texttt{k-means++} performs tenfold or even better in most data sets. These are all properties we look for in an ideal algorithm. However, there are some concerns we raise in the next section.
	\subsection{\texttt{k-means++} vs \texttt{ORSS}}
	Surprisingly, there is no mention of Ostrovsky's algorithm in \texttt{k-means++} paper either even though it was published in $2007$ whereas \cite{ostrovsky} was published in $2006$. In fact, after going through \texttt{ORSS} paper again, we argue that \texttt{k-means++} algorithm is actually a special case of \texttt{ORSS} algorithm. In \texttt{k-means++}, the first center is chosen at random. Then every center is chosen based on the minimum distance of the existing centers from the point in consideration, which \cite{kmeans++} call $D^2$ \textit{weighting}. Now, \texttt{ORSS} chooses two centers $x,y$ with probability proportional to $\|x-y\|^2$. After choosing those two centers, a new center is added to the set of centers in each iteration until there are $k$ centers, which is done exactly as in \texttt{k-means++}. So, the center adding step is same for both \texttt{k-means++} and \texttt{ORSS}. Considering that \texttt{ORSS} was published before \texttt{k-means++}, we want to give credit to \cite{ostrovsky} more than \cite{kmeans++} for this novel approach.
	
	Next, we talk about the first step where \texttt{k-means++} chooses first center at random and \texttt{ORSS} chooses two centers with probability proportional to $\|x-y\|^2$. At this point, we would like to say that our proposed method of improving \texttt{k-means++} is not novel at all. In fact, this was already discussed in \citep{ostrovsky}. However, we did not know that at the time because we did not read the full paper back then. It was only after we discovered that \texttt{k-means++} results could be wrong, we went through \texttt{ORSS} paper again. This time we paid more attention to it and we consider ourselves lucky to have discovered such a gem. We would really like to commend \cite{ostrovsky} for such an impressive work and it is indeed very unlucky of them not getting the credit they deserve. When we read their paper for the second time, we found out that they had already mentioned a result \cite[Page 4, section 3, Paragraph: Running Time]{ostrovsky} similar to ours in their paper. They say that choosing two centers $x,y$ as in \texttt{ORSS} is the same as choosing the first center $x$ with probability proportional to $\sum_{y\in X}\|x-y\|^2$ and the second center with probability proportional to $\|y-c_1\|^2$. As we will see in section \ref{sec:motivation} that the first step was exactly our idea of improving \texttt{k-means++}. Despite being a duplicate result, we have decided to discuss our reasoning in this paper because we believe our motivation was different than theirs. As for the second step, again, notice that this is the same as second step in \texttt{k-means++}. In \texttt{k-means++}, after the first center has been chosen randomly, a point $x$ actually gets chosen with probability $\|x-c_1\|^2$. The reason is, there is only one center so the minimum distance is the only distance from $c_1$ to $x$. 
	
	Moreover, including \texttt{ORSS} makes the results very interesting. \texttt{ORSS} is like a dark horse. There are cases where \texttt{ORSS} is better than both \texttt{k-means++} and our proposed initialization methods. Therefore, we conclude that \texttt{ORSS} is actually an improved version of \texttt{k-means++}.
	\subsection{\texttt{k-means++} Results Are Wrong?}
	We will now discuss the other issue at hand. Regarding the accuracy of \texttt{k-means++} results, we have two concerns. One is that the theoretical results are not correct. And the other is that experimental results might not be correct either.
	
	First, let us discuss the theoretical concern. In the paper \cite{kmeans++}, $D(x)$ is assumed to be the minimum of squared distances  from $x$ to the centers $\C$. In \citep[Lemma $3.3$, section $3$]{kmeans++}, for two distinct point $a,a_0\in A$, the authors use \textit{triangle inequality}\footnote{Triangle inequality states that for any triangle $ABC$, we have $\|AB\|+\|BC\|\geq \|AC\|$. This transforms into $|a|+|b|\geq |a+b|$ for real numbers, consequently $|a|+|x-a|\geq |x|$ holds as well.} for expressing $D(a)^2$ in terms of $D(a_0)$ and $\|a-a_0\|$. Check the statement of \citet[Lemma $3.3$]{kmeans++}.
		\begin{lemma}
			Let $A$ be an arbitrary cluster in $\C_{opt}$ and let $\C$ be an arbitrary clustering. If we add a random center to $\C$ from $A$, chosen with $D^2$ weighting, then $E[\phi(A)] \leq 8\phi_{opt}(A)$.
		\end{lemma}
	The statement seems a bit vague to us for the reason mentioned below. Therefore, it is possible that we did not fully understand what they meant in this lemma. However, based on our interpretation, this lemma is wrong.
	
	They claim in the proof that $D(a_0)\leq D(a)+\|a-a_0\|$. Our concern is whether this is necessarily true or not. First, they take an arbitrary cluster $A$ from the optimal set of centers $\C_{opt}$. Then they choose a point $a_0$ from $A$ with $D^2$ weighting to be added to $\C$, which is an arbitrary clustering. This is the part where our argument lies. Since a center is being added to $\C$ and not $\C_{opt}$, here $D(a)$ is the minimum distance from $a$ to $\C$. If we consider $D(a)=\min(\|a-\C_{opt}\|)$, then the inequality $D(a_0)\leq D(a)+\|a-a_0\|$ holds true. However, in this case $D(a)=\min(\|a-\C\|)$ should be used instead since the center is being added to $\C$. By definition $D(x)$ is the minimum distance from $x$ to the centers in $\C$ and not $\C_{opt}$. Therefore in the inequality, $D(a)$ and $D(a_0)$ are not distances from the same center and hence, it is not directly implied by triangle inequality. We would like to correct the authors of \texttt{k-means++} that the inequality $\dfrac{a^2+b^2}{2} \geq \left(\dfrac{a+b}{2}\right)^2$ is not the \textit{power mean inequality}. This inequality can be derived in many ways, including power mean inequality, but it itself is not power mean inequality. Power mean inequality states that for any $r\leq s$ and $n$ real numbers $a_1,\cdots,a_n$,
		\begin{align*}
			\left(\dfrac{a_1^r+a_2^r+\cdots+a_n^r}{n}\right)^{\frac{1}{r}} & \leq \left(\dfrac{a_1^s+a_2^s+\cdots+a_n^s}{n}\right)^{\frac{1}{s}}
		\end{align*}
	Since we tried to improve \texttt{k-means++}, obviously we made our own implementations of the algorithm and checked the results against the ones shown in the paper. Our results conflict with some results shown in \texttt{k-means++} paper \cite{kmeans++}. For this reason, we ran clustering on the same data set (cloud data set) using \texttt{k-means} package from \texttt{scikit-learn} library \citep{sklearn}. We found out that our implementations achieve similar inertia to the one in \texttt{scikit-learn}. On the other hand, results shown in \texttt{k-means++} paper differ from both our and \texttt{scikit-learn} results. At first, we thought this might be due to normalization or standardization. However, even after using both \texttt{min max} and \texttt{standard} normalization (also known as $z$-score), we found out that \texttt{k-means++} results still do not match with neither of the results. Check that both our and \texttt{scikit-learn} implementations have similar intertia values in tables \ref{tbl:cloud_minmaxscaler} (min-max scalar used to process data), \ref{tbl:cloud_no_preprocess} (no preprocessing) and \ref{tbl:cloud_standardscaler} (standard scalar pre-processing). But \texttt{k-means++} results do not agree with ours \citep[Table $3$]{kmeans++}. Therefore, we are forced to believe that either we did something very wrong or they did or there is some difference in the data set. We confirmed that our data set consisted of $1024$ rows and had dimension $10$. 
	
	We also tried collecting every data set that is used in \texttt{k-means++} paper but we managed to get only the cloud data set. We could not find the other real world data set \textit{Intrusion} data set. There were some data sets named intrusion detection data set but they did matched neither the number of data points (494019) nor the dimension (35). Therefore, cloud data set was our primary source of comparison. We would like to mention that in a footnote in \cite[Section $6$, page $8$]{kmeans++}, the authors mentioned that \url{http://theory.stanford.edu/~sergei/kmeans} contained full test suite. However, the site seems to be no longer accessible even though \url{http://theory.stanford.edu/~sergei} is. Therefore, we could not retrieve the original data sets used in their paper or check their implementation.
	
	We would also like to mention that we emailed the second author for both concerns and we have yet to receive any reply.
	\section{Proposed Initialization Methods}
	For a set of points $S$ and a point $x$, we use $\min(\|x-S\|)$ to denote the minimum of distances from $x$ to the points of $S$ that is $\min(\| x-S\|)=\min_{a\in S}(\| x-a\|)$. Set $D(\x)=\min(\|\x-\C\|)$ for a point $x$ and a set of centers $\C$. Let us denote the centroid of $S$ by $\mu_S$ that is $\mu_S=\dfrac{1}{|S|}\sum_{x\in S}x$.
	\subsection{First center for \texttt{k-means++}}\label{sec:first_center}
	In \texttt{k-means++} algorithm, the first center is chosen uniformly at random. However, not all points have the same contribution to inertia. We choose $x$ as a first center in a way that is equivalent to the variance explained by $x$.
	\begin{enumerate}[i]
		\item Choose $\x$ with probability $\dfrac{\|\x-\mu_{\X}\|^2}{\sum_{\x\in\X}\|\x-\mu_{\X}\|^2}$. Set $\C_1=\{\x\}$.\label{step:first_center}
		\item Repeat the remaining steps in \texttt{k-means++} \cite[Section $2.2$, Page $3$]{kmeans++}.
	\end{enumerate}
	\subsection{Centroid of Centers Based Seeding}
	We want to choose $x$ with probability proportional to squared distance from centroid of the cluster centers. Our motivation for doing so is the following. In \texttt{k-means++}, probability is considered proportional to squared minimum distance from the centers. Therefore, the larger this minimum squared distance is, the higher the probability is for $x$ to be chosen as a center. So, in a sense this can be thought of maximizing the minimum squared distance from the centers to the point in consideration. We intend to check the case where we choose the probability proportional to the total sum of squared distances rather than just the minimum one. As we will show later, sum of all squared distances from centers to the point in discussion is dependent on the distance from centroid of those cluster centers to that point.
		\begin{enumerate}[i]
			\item Choose a point $\x$ as stated in step \eqref{step:first_center} of section \eqref{sec:first_center}. Set $\C_1=\{\x\}$.
			\item For an already existing set of $i$ centers $\mathcal{C}_i=\{c_1,\cdots,c_i\}$, choose a new center $\x\in\X$ with probability proportional to $\|\x-\mu_{\C_i}\|^2$.\label{step:coc_center}
			\item Repeat step \eqref{step:coc_center} until $i=k$.
			\item For each $1\leq i\leq k$, set $\C_i=\{\x\in\X:\|\x-c_i\|=\min(\x-\C)\}$.\label{step:coc_cluster}
			\item Set $c_i=\mu_{\C_i}$.\label{step:coc_update}
			\item Repeat \eqref{step:coc_cluster} and \eqref{step:coc_update} until convergence is reached.
		\end{enumerate}
	\section{Motivation}\label{sec:motivation}
	In this section, we discuss the motivation behind our algorithm and why they make sense mathematically. 
	Consider a set of $n$ points $\X$ and that the probability of $x\in\X$ being chosen as a center as $p(x)$. Following the definition of variance, for a set of points $S$, we define
		\begin{align}
			\sigma^2(S) & = \dfrac{\sum_{x\in S} \|x-\mu_{S}\|^2}{|S|}\nonumber\\
			\sum_{x\in S}\|x-\mu_{S}\|^2 & = |S|\sigma^2\label{eqn:1}
		\end{align}
	For any arbitrary point $a$ and $\mu$ as the centroid of $\X$,
		\begin{align}
			\sum_{\x\in\X}\|\x-a\|^2
				  & = \sum_{\x\in\X}\|\x-\mu+\mu-a\|^2\nonumber\\
				  & = \sum_{\x\in\X}\left(\|\x-\mu\|^2+2\langle\x-\mu,\mu-a\rangle+\|\mu-a\|^2\right)\nonumber\\
				  & = \sum_{\x\in\X}\|\x-\mu\|^2+2\left\langle\sum_{\x\in\X}\x-n\mu,\mu-a\right\rangle+n\|\mu-a\|^2\nonumber\\
				  & = n\sigma^2+2\langle n\mu-n\mu,\mu-a\rangle+n\|\mu-a\|^2\nonumber\\
				  & = n(\sigma^2+\|\mu-a\|^2)\label{eqn:2}
		\end{align}
	Here $\langle a,b\rangle$ is the dot product of vectors $a$ and $b$. Using equation \eqref{eqn:2}, we have the following.
		\begin{align}
			\sum_{\x\in\X}\sum_{\y\in\X}\|\x-\y\|^2 
				& = \sum_{\x\in\X}n(\sigma^2+\|\mu-\x\|^2)\nonumber\\
				& = n(n\sigma^2+\sum_{\x\in\X}\|\mu-\x\|^2)\nonumber\\
				& = n(n\sigma^2+n\sigma^2)\nonumber\\
				& = 2n^2\sigma^2\label{eqn:3}
		\end{align}
	Using equation \eqref{eqn:3}, the probability becomes
		\begin{align*}
			p(x) & = \dfrac{\sum_{\y\in\X}\|\x-\y\|^2}{\sum_{\y\in\X}\sum_{\x'\in\X}\|\x'-\y\|^2}\\
				 & = \dfrac{n(\sigma^2+\|\mu-\x\|^2)}{2n^2\sigma^2}\\
				 & = \dfrac{\sigma^2+\|\mu-\x\|^2}{2n\sigma^2}\\
				 & = \dfrac{1}{2n}+\dfrac{\|\mu-\x\|^2}{2n\sigma^2}
		\end{align*}
	However, to make things smoother, one can also choose to use the following as the probability of $x$ being chosen as the first center.
		\begin{align*}
			p(x) & = \dfrac{\|\mu-\x\|^2}{n\sigma^2}
		\end{align*}
	Notice that the variance of $S$ is in the denominator. We can consider this as the amount of variance explained by $x$. Also, notice that computationally this version of $p(x)$ is much cheaper than using $\sum_{\y\in\X}\|\x-\y\|^2$. Therefore, we considered $p(x)$ proportional to $\|x-\mu\|^2$ for choosing $x$ as the first center in our experiments.
	\section{The Inertia Theorem}
	%Since the algorithms in discussion are probabilistic, we should look at the expected value of inertia. For a set of points $\X$ and a set of centers $\C$, we denote the inertia by $\I_\C(\X)$. %For the optimal set of cluster centers $\C_{opt}$, we denote the corresponding inertia by $\I_{opt}(\X)$.
		%align*}
	For the minimum distance from $D(x)$ from $x$ to the set of points $\C$,
		\begin{align*}
			D(x)^2 & \leq \dfrac{1}{k}\sum_{c\in \C}\|x-c\|^2\\
					& = \dfrac{1}{k}\left(k\|x-\mu_{\C}\|^2+\sum_{c\in\C}\|\mu_{\C}-c\|^2\right)\\
					& = \|x-\mu_{\C}\|^2+\dfrac{1}{k}\sum_{c\in\C}\|\mu_{\C}-c\|^2
		\end{align*}
	Thus, we can write inertia as 
		\begin{align*}
			I & = \sum_{x\in S}\min_{c\in C}\|x-c\|^2\\
			D(x)^2 & = \min_{c\in C}\|x-c\|^2\\
			D(x)^2 & \leq \|x-\mu_{C}\|^2+\dfrac{1}{k}\sum_{c\in C}\|\mu_{C}-c\|^2\\
			I & \leq \sum_{x\in S}\left(\|x-\mu_C\|^2+\dfrac1kI_C\right)\\
			& = \sum_{x\in S}\|x-\mu_C\|^2+\dfrac nkI_C\\
			& = \sum_{x\in S}\|x-\mu\|^2+n\|\mu-\mu_C\|^2+\dfrac nkI_C
		\end{align*}
	This holds for any seeding technique.
		\begin{theorem}
			For a set of points $S$ and a set of centers $C$, we have
				\begin{align*}
					\I(S) & \leq n(\sigma^2+\|\mu_S-\mu_C\|^2)+\dfrac{n}{k}\I_{opt}(C)
				\end{align*}
			regardless of what seeding method is used.
		\end{theorem}
	\section{Experiment Setup}
	We ensure that all algorithms are run under the same conditions. All of them share the same environment and no special optimizations were made for any particular algorithm. Only CPU was used to determine the values we are interested in and no parallelism mechanism was in place for speeding up the process. This way, we can get an idea about the raw performances of the algorithms involved.
	
	Python is used as the programming language to write necessary codes. Some common auxiliary packages such as \textit{scipy, scikit-learn, numpy} etc are used to help with the code. The algorithms are simply different methods of the same class, so they share the same fitting and prediction function. Only the initialization differs for different algorithm. It should be mentioned that even though some packages have native support for \texttt{k-means} implementation, we did not use them to run the experiments. Not all algorithms we want to test are available in those packages. Therefore, in order to ensure same environment and optimizations for every algorithm, we wrote them all from scratch so that we could be sure they are tested under the same settings.
	
	The data sets used for the experiment are some of the popular ones.
		\begin{enumerate}
			\item Boston housing data set
			\item Wine quality testing data set
			\item Mall customers data set
			\item Airlines cluster data set
			\item Cloud data set
			\item Two moons data set
			\item Boston schools data set
			\item Old faithful geyser data set
			\item Iris data set
		\end{enumerate}
	\cite{miligan} shows that using $z$-score to standardize the data is not favorable for clustering because it loses between-cluster variation. Therefore, we did not use any sort of standardization or normalization lest it should lose variance or become prone to bias. Instead, we have used PCA to remove linear dependency among variables. 
	
	While experimenting on such algorithms, it is of utmost importance to run the same experiment more than once under the same parameters and conditions. For example, assume that we want to compare \texttt{k-means++} and Forgy's algorithm \citep{forgy} for $k=5$ clusters. We should run this experiment at least $m$ times where $m>1$ in order to eliminate bias and account for randomness. We run each experiment a total of $20$ times and take the average and minimum values of inertia.
	
	As we will see in section \eqref{sec:results}, usually every method can reach the minimum inertia at least once. Therefore, the method with lower average performs more consistently without any doubt. However, there are also cases where the difference between default \texttt{k-means} and seeded \texttt{k-means} becomes obvious.
	\section{Results}\label{sec:results}
	We present the results on the initialization procedures mentioned before. We have shown the results for $k\in\{5,10,25\}$ clusters. The ceiling of average number of iterations is taken since it may not be an integer. The time registered here is the average CPU time taken by each algorithm for those $20$ iterations. As expected, default \texttt{k-means} is usually the fastest algorithm but also the worst in terms of optimizing inertia.
	\subfile{kmeans-comparison}

		\begin{table}
			\begin{center}
				\begin{tabular}{|l|l|l|}
					\hline
					Number of clusters & Inertia (scikit-learn) & Inertia (our)\\\hline
					5 & 17706689.573774982& 17711385.61029025\\\hline
					10 & 5761674.929143367& 6434641.098051261\\\hline
					25 & 2007444.7098438586& 2255902.212313077\\\hline
					50 & 1099395.4420880969& 1194727.658029051\\\hline
				\end{tabular}
				\caption{\texttt{k-mean++} results on cloud data set}
				\label{tbl:cloud_no_preprocess}
			\end{center}
		\end{table}

		\begin{table}
			\begin{center}
				\begin{tabular}{|l|l|l|}
					\hline
					Number of clusters & Inertia (scikit-learn) & Inertia (our)\\\hline
					5 & 2519.79170884024 & 2519.826060045223\\\hline
					10 & 1521.9262422548954 & 1510.9057079504264\\\hline
					25 & 817.4296830232086 & 833.2041864088023\\\hline
					50 & 519.1784696946825 & 543.6392985861094\\\hline
				\end{tabular}
				\caption{\texttt{k-means++} results on cloud data set (standardized)}
				\label{tbl:cloud_standardscaler}
			\end{center}
		\end{table}

		\begin{table}
			\begin{center}
				\begin{tabular}{|l|l|l|}
					\hline
					Number of clusters & Inertia (scikit-learn) & Inertia (our)\\\hline
					5 & 57.096454766857384 & 57.302792493848216\\\hline
					10 & 32.929845052888986 & 32.93489472987519\\\hline
					25 & 17.941355048689374 & 18.50259622163847\\\hline
					50 & 11.362562342560503 & 11.7818373139353\\\hline
				\end{tabular}
				\caption{\texttt{k-means++} results on cloud data set (scaled)}
				\label{tbl:cloud_minmaxscaler}
			\end{center}
		\end{table}

	\section{Conclusion}
	We proposed methods to improve current state of \texttt{k-means} algorithms and showed their comparison in terms of convergence and inertia. \texttt{k-means++} is usually worse than both our proposed algorithm and \texttt{ORSS}.\clearpage
	\subfile{bib}
\end{document}