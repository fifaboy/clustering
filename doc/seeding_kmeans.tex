\documentclass[twoside, 11pt]{article}

\usepackage{jmlr2e}
\usepackage{amsmath, amsfonts, enumerate, color}

%\usepackage{amsthm, amssymb, amsfonts, amsmath, geometry, enumerate}
%\geometry{left=1in,right=1in,top=.5in,bottom=1in,headheight=4pt,headsep=0.3in}


\newcommand{\x}{\mathbf{x}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\z}{\mathbf{z}}
\renewcommand{\c}{\mathbf{c}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\I}{\mathcal{I}}
\renewcommand{\S}{\mathcal{S}}

\title{Seeded Center Initialization In $k$-Means Clustering}
\author{\name Masum Billal \email billalmasum93@gmail.com \\
	Senior Data Scientist\\
	\addr Data Science Department\\
	Shohoz\\
	Bangladesh}
\editor{Not assigned}
\begin{document}
	\bibliographystyle{plain}
	\maketitle
		\begin{abstract}%
			$k$-Means is a popular clustering algorithm that aims to reduce the sum of minimum squared distances from data points to the centers. Now a days most of the times seeded centers are used instead of choosing all uniformly at random. $k$-Means++ chooses the centers with a so called $D^2$ weighting. However, that is only one of the ways. Similar ideas are possible for seeding the centers. In this paper, we improve $k$-Means++ by choosing the first center with a probability instead of selecting a random point and we analyze another way of initializing the centers. Empirical evidence shows that our proposed algorithms perform better than available methods of center initialization consistently.
		\end{abstract}
	\section{Introduction}
	Widely regarded as the most popular clustering techniques, $k$-Means remains a humble interesting topic in machine learning as well as computational geometry. Roughly the problem is: given a set of points $\X$ in $\mathbb{R}^d$. Find a set of centers $\mathcal{C}$ such that the function \textit{inertia}
		\begin{align*}
			\mathcal{I} & = \sum_{\x\in\X}\min_{\c\in\C}(\|\c-\x\|^2)
		\end{align*}
	is minimum where $\|\cdot\|$ is the $L_2$ norm\footnote{$\|c-x\|$ or $L_2$ norm of $\c-\x$ is the distance between the center $\c$ and point $\x$ or the magnitude of the vector $\c-\x$.}.
	
	Default $k$-Means algorithm starts with random centers and then converge based on minimum distances of the centers from the data points. New centers are calculated based on the centroid. This is known as Lloyd's algorithm \citep{lloyd}. We repeat this process until no more change is possible. \cite{ostrovsky} and \cite{kmeans++} take it one step further by choosing the initial centers with a probability. We intend to introduce other ways of initialization.
	\acks{I would like to thank my colleague, Farhad Naeem for pointing out a careless mistake I made in writing the formula of variance.}
	\section{Related Work}
	There has been multiple surveys on $k$-Means in the literature. Probably the most relevant work in this regard is done by \cite{celebi}. However, most of the algorithms used in that paper are not used practically very much. It was also noted by the authors themselves that $k$-Means++ and its greedy version work better than most. They also mention that probabilistic algorithms perform better than deterministic ones. Moreover, another highly influential center initialization algorithm \citep{ostrovsky} was not considered in their experiments. There is no mention of Ostrovsky's algorithm in their paper whatsoever. Surprisingly, there is no mention of Ostrovsky's algorithm in $k$-Means++ paper either even though it was published in $2007$ whereas Ostrovsky's algorithm was published in $2006$.
	
	We would also like to point out that to our knowledge no surveys were done after removing linear dependency prior to running the experiments. This is a very important step if we are to get a meaningful clustering out of $k$-Means algorithm. PCA also helps with some drawbacks of $k$-Means such as $k$-Means struggles with non-spherical data set. PCA decomposes the existing data points into orthogonal\footnote{It is well known that orthogonal vectors are linearly independent.} ones. If we use PCA (principal component analysis) before running a clustering algorithm, we can redefine the variables into linearly independent ones. For our experiment, we have used PCA on every data set before running cluster algorithm.
	\section{Proposed Initialization Methods}
	For a set of points $S$ and a point $x$, we use $\min(\|x-S\|)$ to denote the minimum of distances from $x$ to the points of $S$ that is $\min(\| x-S\|)=\min_{a\in S}(\| x-a\|)$. Set $D(\x)=\min(\|\x-\C\|)$ for a point $x$ and a set of centers $\C$. Let us denote the centroid of $S$ by $\mu_S$ that is $\mu_S=\dfrac{1}{|S|}\sum_{x\in S}x$.
	\subsection{First center for $k$-Means++}
	In $k$-Means++ algorithm, the first center is chosen uniformly at random. However, not all points have the same contribution to inertia. We choose $x$ as a first center in a way that is equivalent to the variance explained by $x$.
	\begin{enumerate}[i]
		\item Choose $\x$ with probability $\dfrac{\|\x-\mu_{\X}\|^2}{\sum_{\x\in\X}\|\x-\mu_{\X}\|^2}$. Set $\C_1=\{\x\}$.\label{step:first_center}
		\item Repeat the remaining steps in $k$-Means++ \cite[Section $2.2$, Page $3$]{kmeans++}.
	\end{enumerate}
	\subsection{Centroid of Centers Based Seeding}
	We want to choose $x$ with probability proportional to squared distance from centroid of the cluster centers. Our motivation for doing so is the following. In $k$-Means++, probability is proportional to squared minimum distance from the centers. Therefore, the larger this minimum squared distance is, the higher the probability is for $x$ to be chosen as a center. So, in a sense this can be thought of maximizing the minimum squared distance from the centers to the point in consideration. We intend to check the case where we choose the probability proportional to the total sum of squared distances rather than just the minimum one.
		\begin{enumerate}[i]
			\item Choose a centers $\x$ like the first center for $k$-Means++ as stated in\eqref{step:coc_center}. Set $\C_1=\{\x\}$.
			\item For an already existing set of $i$ centers $\mathcal{C}_i=\{c_1,\cdots,c_i\}$, choose a new center $\x\in\X$ with probability proportional to $\|\x-\mu_{\C_i}\|^2$.\label{step:coc_center}
			\item Repeat step \eqref{step:coc_center} until $i=k$.
			\item For each $1\leq i\leq k$, set $\C_i=\{\x\in\X:\|\x-c_i\|=\min(x-\C)\}$.\label{step:coc_cluster}
			\item Set $c_i=\mu_{\C_i}$.\label{step:coc_update}
			\item Repeat \eqref{step:coc_cluster} and \eqref{step:coc_update} until convergence or number of iteration is reached.
		\end{enumerate}
	\section{Analysis}
	First, we will analyze the choosing of first center in $k$-Means++. 
	\subsection{$k$-Means++ Improved}
	Consider a set of $n$ points $\X$ and that the probability of $x\in\X$ being chosen as a center as $p(x)$. From the definition of variance, for a set of points $S$,
		\begin{align}
			\sigma^2(S) & = \dfrac{\sum_{x\in S} \|x-\mu_{S}\|^2}{|S|}\nonumber\\
			\sum_{x\in S}\|x-\mu_{S}\|^2 & = |S|\sigma^2\label{eqn:1}
		\end{align}
	For any arbitrary point $a$ and $\mu$ as the centroid of $\X$,
		\begin{align}
			\sum_{\x\in\X}\|\x-a\|^2
				  & = \sum_{\x\in\X}\|\x-\mu+\mu-a\|^2\nonumber\\
				  & = \sum_{\x\in\X}\left(\|\x-\mu\|^2+2\langle\x-\mu,\mu-a\rangle+\|\mu-a\|^2\right)\nonumber\\
				  & = \sum_{\x\in\X}\|\x-\mu\|^2+2\left\langle\sum_{\x\in\X}\x-n\mu,\mu-a\right\rangle+n\|\mu-a\|^2\nonumber\\
				  & = n\sigma^2+2\langle n\mu-n\mu,\mu-a\rangle+n\|\mu-a\|^2\nonumber\\
				  & = n(\sigma^2+\|\mu-a\|^2)\label{eqn:2}
		\end{align}
	Using equation \eqref{eqn:2}, we have the following.
		\begin{align}
			\sum_{\x\in\X}\sum_{\y\in\X}\|\x-\y\|^2 
				& = \sum_{\x\in\X}n(\sigma^2+\|\mu-\x\|^2)\nonumber\\
				& = n(n\sigma^2+\sum_{\x\in\X}\|\mu-\x\|^2)\nonumber\\
				& = n(n\sigma^2+n\sigma^2)\nonumber\\
				& = 2n^2\sigma^2\label{eqn:3}
		\end{align}
	Using equation \eqref{eqn:3}, the probability becomes
		\begin{align*}
			p(x) & = \dfrac{\sum_{\y\in\X}\|\x-\y\|^2}{\sum_{\y\in\X}\sum_{\x'\in\X}\|\x'-\y\|^2}\\
				 & = \dfrac{n(\sigma^2+\|\mu-\x\|^2)}{2n^2\sigma^2}\\
				 & = \dfrac{\sigma^2+\|\mu-\x\|^2}{2n\sigma^2}\\
				 & = \dfrac{1}{2}+\dfrac{\|\mu-\x\|^2}{2n\sigma^2}
		\end{align*}
	However, to make things smoother, one can also choose to use the following as the probability of $x$ being chosen as the first center.
		\begin{align*}
			p(x) & = \dfrac{\|\mu-\x\|^2}{2n\sigma^2}
		\end{align*}
	Notice that the denominator is the variance of $S$. Therefore, we can consider this as the amount of variance explained by $x$. Also, notice that computationally this version of $p(x)$ is much cheaper than using $\sum_{\y\in\X}\|\x-\y\|^2$. Therefore, we considered $p(x)$ proportional to $\|x-\mu\|^2$ for choosing $x$ as the first center in our experiments.
	\subsection{Centroid of Centers Seeding}
	We will now analyze the center initialization algorithms using inertia value. Since these algorithms are probabilistic, we are going to take a look at the expected value of inertia. For a set of points $\X$ and a set of centers $\C$, we denote the inertia by $\I_\C(\X)$. For the optimal set of cluster centers $\C_{opt}$, we denote the corresponding inertia by $\I_{opt}(\X)$.
		\begin{align*}
			\I_\C(\X) & = \sum_{\x\in\X}\min_{\c\in\C}(\|c-x\|^2)
		\end{align*}
	If the context is clear, we may omit $\C$ and $\X$. For a fixed set of points $\X$, if the probability of $\x\in\X$ being chosen to be a center is $p(x)$ with respect to a set of centers $\C$, then the expected value of inertia $E[\I(\X)]$ is
		\begin{align*}
			E[\I(\X)] & = \sum_{\x\in\X}p(x)\sum_{\y\in\X}\min(D(x),\|y-x\|)^2
		\end{align*}
	The following lemma was proven in \cite[Lemma $3.2$]{kmeans++}.
		\begin{lemma}
			Let $A$ be an arbitrary cluster in $\C_{opt}$, and let $\C$ be the clustering with just one center, which is chosen uniformly at random from $A$. Then, $E[\I(A)]=2\I_{opt}(A)$.
		\end{lemma}
	Here, $\I_{opt}(A)=\sum_{x\in A}\|x-\mu_A\|^2$. 
	
	Now, we want to find a similar formula for the remaining centers using centroid of centers seeding. If we take $p(x)$ proportional to $f(x)=\|x-\mu_{\C}\|^2$, then using equation \eqref{eqn:2},
		\begin{align*}
			f(x) & = \|x-\mu_{\C}\|^2\\
			\sum_{x\in S}f(x) 
				& = \sum_{x\in S}\|x-\mu_{\C}\|^2\\
				& = |S|(\sigma^2+\|\mu_S-\mu_{\C}\|^2)
		\end{align*}
	Then the probability of $x$ being chosen as a center is
		\begin{align*}
			p(x) & = \dfrac{f(x)}{\sum_{y\in S}f(y)}\\
				 & = \dfrac{\|x-\mu_{\C}\|^2}{|S|(\sigma^2+\|\mu_{S}-\mu_{\C}\|^2)}
		\end{align*}
	Since $D(y)\leq \|y-\mu_{\C}\|$, we have the following
		\begin{align*}
			\sum_{y\in S}\|y-\mu_{\C}\|^2 & \geq D(y)^2\\
			\dfrac{1}{\sum_{y\in S}\|y-\mu_{\C}\|^2} & \leq \dfrac{1}{\sum_{y\in S}D(y)^2}\\
			\dfrac{\sum_{y\in S}D(y)^2}{\sum_{y\in S}\|y-\mu_{\C}\|^2} & \leq 1
		\end{align*}
	Our expected value of inertia would be
		\begin{align*}
			E[\I(S)] & = \sum_{x\in S}p(x)\sum_{y\in S}\min(D(y),\|x-y\|)^2\\
					   & = \sum_{x\in S}\dfrac{\|x-\mu_{\C}\|^2}{\sum_{y\in S}\|y-\mu_{\C}\|^2}\sum_{y\in S}\min(D(y),\|x-y\|)^2\\
					   %& \leq \sum_{y\in S}\dfrac{1}{n}\dfrac{\sum_{y\in S}2(\|x-y\|^2)+2(\|y-\mu_{\C}\|^2)}{\sum_{y\in S}\|y-\mu_{\C}\|^2}\sum_{y\in S}\min(D(y),\|x-y\|)^2\\
					   & \leq \sum_{x\in S}\dfrac{\|x-\mu_{\C}\|^2}{\sum_{y\in S}\|y-\mu_{\C}\|^2}\sum_{y\in S}D(y)^2\\
					   & \leq \sum_{x\in S}\|x-\mu_{\C}\|^2
		\end{align*}
	Using equation \eqref{eqn:2}, we have
		\begin{align*}
			E[\I(S)] & \leq n(\sigma^2+\|\mu_S-\mu_{\C}\|^2)
		\end{align*}
	Now, using equation \eqref{eqn:3}, we have
		\begin{align*}
			\I_{opt}(S) & = \sum_{x\in S}\|x-\mu_{S}\|^2\\
						  & = n\sigma^2
		\end{align*}
	Thus, we have the following theorem.
		\begin{theorem}
			Let $S$ be an arbitrary cluster chosen from $\C_{opt}$ and $\C$ be an arbitrary clustering. If a center is chosen from $S$ and added to $\C$ using centroid of centers initialization, then
				\begin{align*}
					E[\I(S)] & \leq \I_{opt}(S)+n\|\mu_{S}-\mu_{\C}\|^2
				\end{align*}
		\end{theorem}
	For a set of points $S$, consider the partition into $k$ clusters $\{S_1,S_2,\cdots,S_k\}$ so that $S=S_1\cup S_2\cup\cdots\cup S_k$ and $S_i\cap S_j=\{\}$ for $i\neq j$. For brevity, we denote the center of $S_i$ by $\mu_i$ instead of $\mu_{S_i}$ and the center of $S$ by $\mu$. Also, assume that $|S_i|=n_i$ for each $1\leq i\leq k$ and that the set of centers of the clusters is $\mathcal{C}=\{\mu_1,\mu_2,\cdots,\mu_k\}$. We have $\sum_{x\in S_i}\|x-\mu_i\|^2 = n(\sigma_i^2+\mu_i^2)$ where $\sigma_i$ is the variance of the cluster $S_i$.
		\begin{align}
			\sum_{x\in S}\|x-\mu\|^2 
				& = \sum_{i=1}^k\sum_{x\in S_i}\|x-\mu\|^2\nonumber\\
			n\sigma^2
				& = \sum_{i=1}^kn_i(\sigma_i^2+\|\mu_i-\mu\|^2)\nonumber\\
				& = \sum_{i=1}^kn_i\sigma_i^2+\sum_{c\in\C}\|c-\mu\|^2\nonumber\\
				& = \sum_{i=1}^kn_i\sigma_i^2+k(\sigma_{\C}^2+\|\mu_{\C}-\mu\|^2)\label{eqn:4}\\
		\end{align}
	Therefore, $k\|\mu-\mu_{\C}\|^2\leq n\sigma^2$ and we have
		\begin{align*}
			E[\I(S)] & \leq \I_{opt}(S)+n\cdot\dfrac{n\sigma^2}{k}\\
					 & \leq \I_{opt}(S)+n\dfrac{\I_{opt}(S)}{k}
		\end{align*}
	This gives us the following result.
		\begin{theorem}
			$E[\I(S)]\leq \I_{opt}\left(1+\dfrac{n}{k}\right)$.
		\end{theorem}
	We believe this estimate is not tight at all and it can be improved by a huge margin. For example, we had ideas of using Abel's summation formula \citep[Theorem $4.2$]{apostol} and Taylor series to approximate the sum on the right side of equation \eqref{eqn:4} and get a tighter bound. However, we were happy with the experimental results and therefore, we decided to not spend any more time in finding a tighter bound. We may work on this in future, but right now we do not have any plans to improve this bound.
	\section{Experiment Setup}
	We ensure that all algorithms are run under the same conditions. All of them share the same environment and no special optimizations were made for any particular algorithm. Only CPU was used to determine the values we are interested in and no parallelism mechanism was in place for speeding up the process. This way, we can get an idea about the raw performances of the algorithms involved.
	
	Python is used as the programming language to write necessary codes. Some common auxiliary packages such as \textit{scipy, scikit-learn, numpy} etc are used to help with the code. The algorithms are simply different methods of the same class, so they share the same fitting and prediction function. Only the initialization differs for different algorithm. It should be mentioned that even though some packages have native support for $k$-Means implementation, we did not use them to run the experiments. Not all algorithms we want to test are available in those packages. Therefore, in order to ensure same environment and optimizations for every algorithm, we wrote them all from scratch so that we could be sure they are tested under the same settings.
	
	The data sets used for the experiment are some of the popular ones.
		\begin{enumerate}
			\item Boston housing data set
			\item Wine quality testing data set
			\item Mall customers data set
			\item Airlines cluster data set
			\item Iris flower data set
			\item Cloud data set
		\end{enumerate}
	\cite{miligan} shows that using $z$-score to standardize the data is not favorable for clustering because it loses between-cluster variation. Therefore, we did not use any sort of standardization or normalization lest it should lose variance or become prone to bias. Instead, we have used PCA to remove linear dependency among variables. 
	
	While experimenting on such algorithms, it is of utmost importance to run the same experiment more than once under the same parameters and conditions. For example, assume that we want to compare $k$-Means++ and Forgy's algorithm \citep{forgy} for $k=5$ clusters. We should run this experiment at least $m$ times where $m>1$ in order to eliminate bias and account for randomness. We run each experiment a total of $20$ times and take the average and minimum values of inertia.
	
	We would like to address another important issue regarding $k$-Means algorithms. Usually convergence speed is tested for $k$-Means by checking how many iteration it requires until the centers stop changing. However, as we will see, given enough number of iterations all initialization can eventually produce the minimum inertia value. This enabled us to run the experiments a fixed number of times for every initialization instead of checking the convergence. There is another big reason for doing so. Consider some convergence criterion. For example, when the sum of squared distances between the old and new centers is less than some tolerance value, we reach convergence. Using this kind of criterion does not necessarily optimize inertia value as much as possible. In our experiments, we have found multiple instances where convergence is reached in less than 10 iterations but the inertia value is far from the minimum possible value. The speed of convergence is our secondary goal and since that does not actually help us distinguish the initialization procedures, we decided to not use this at all. We have another reason for doing so. If a data set is normalized or standardized for some reason, then the convergence criterion does not work as well either. The reason is that it already has very small distances between old and new centers. It is no wonder convergence would be reached really fast regardless of what initialization is used. And this does not necessarily indicate that the optimum inertia value was achieved. There is another benefit of using this setting. We can understand how consistent an initialization is. As we will see in section \eqref{sec:results}, every method can reach the minimum inertia at least once. Therefore, the method with lower average performed more consistently without any doubt. We will have a similar implication for comparison between $k$-Means++ and our proposed improvement for $k$-Means++.
	\section{Results}\label{sec:results}
	We present the results on the initialization procedures mentioned before. We have let each algorithm run exactly the same number of times ($300$) which is more than enough for convergence. The time registered here is the time taken by each algorithm for those $300$ iterations. As expected, default $k$-Means is usually the fastest algorithm. Also, as mentioned earlier, every method was repeated $20$ times and average/minimum was considered for those iterations.
	\subsection{Comparison of Different Initialization}
	The comparison of inertia for different initialization is shown in tables \ref{tbl:boston}, \ref{tbl:wine}, \ref{tbl:airlines}. We have shown the results here for $k=5$ clusters.
		\begin{table}
			\begin{center}
				\begin{tabular}{|l|l|l|l|}
					\hline
					Algorithm & Average $\I$& Minimum $\I$& Time\\\hline
					$k$-Means & 2586435.45& 1442170.41& 2.68s\\\hline
					$k$-Means++ & 1638992.86& 1442170.41& 2.66s\\\hline
					ORSS & 1620320.77& 1442170.41& 2.69s\\\hline
					CoC & 1562697.24& 1442170.41& 2.66s\\\hline
				\end{tabular}
			\caption{Results on Boston housing data set, $5$ clusters}
			\label{tbl:boston}
			\end{center}
		\end{table}
	
		\begin{table}
			\begin{center}
				\begin{tabular}{|l|l|l|l|}
					\hline
					Algorithm & Average $\I$& Minimum $\I$& Time\\\hline
					$k$-Means & 1011171.27& 916424.19& 0.93s\\\hline
					ORSS & 1020273.2& 916424.19& 0.94s\\\hline
					$k$-Means++ & 1011171.27& 916424.19& 0.94s\\\hline
					CoC & 994075.55& 916424.19& 0.94s\\\hline
				\end{tabular}
				\caption{Results on Wine data set, $5$ clusters}
				\label{tbl:wine}
			\end{center}
			
		\end{table}

		\begin{table}
			\begin{center}
				\begin{tabular}{|l|l|l|l|}
					\hline
					Algorithm & Average $\I$& Minimum $\I$& Time\\\hline
					$k$-Means & 5788612160669.75& 5724491382108.87& 20.22s\\\hline
					ORSS & 5883085483272.44& 5724491382108.87& 20.2s\\\hline
					$k$-Means++ & 5908999306255.42& 5724491382108.87& 20.33s\\\hline
					CoC & 5782820760840.1& 5724491382108.87& 20.2s\\\hline
				\end{tabular}
				\caption{Results on Airlines data set, $5$ clusters}
				\label{tbl:airlines}
			\end{center}
			
		\end{table}
	\subsection{$k$-Means++ vs $k$-Means++ Improved}
	The comparison of $k$-Means++ against $k$-Means++ improved is shown in tables \ref{tbl:cloud}, \ref{tbl:mall2}, \ref{tbl:wine2}.
		\begin{table}
			\begin{center}
				\begin{tabular}{|l|l|l|l|}
					\hline
					Algorithm & Average $\I$& Minimum $\I$& Time\\\hline
					$k$-Means++ & 17693133.44& 17414699.85& 4.95s\\\hline
					$k$-Means++ Improved& 17614961.42& 17414699.85& 4.93s\\\hline
				\end{tabular}
				\caption{Results on Cloud data set, $5$ clusters}
				\label{tbl:cloud}
			\end{center}
		\end{table}
	
		\begin{table}
			\begin{center}
				\begin{tabular}{|l|l|l|l|}
					\hline
					Algorithm & Average $\I$& Minimum $\I$& Time\\\hline
					$k$-Means++ & 89866.67& 78385.08& 0.99s\\\hline
					$k$-Means++ Improved& 88468.3& 78392.42& 0.99s\\\hline
				\end{tabular}
				\caption{Results on Mall customer data set, $5$ clusters}
				\label{tbl:mall2}
			\end{center}
		\end{table}
	
		\begin{table}
			\begin{center}
				\begin{tabular}{|l|l|l|l|}
					\hline
					Algorithm & Average $\I$& Minimum $\I$& Time\\\hline
					$k$-Means++ & 1007402.92& 916424.19& 0.91s\\\hline
					$k$-Means++ Improved& 995907.65& 916424.19& 0.91s\\\hline
				\end{tabular}
				\caption{Results on Wine data set, $5$ clusters}
				\label{tbl:wine2}
			\end{center}
		\end{table}
	Notice that in all cases, we achieved lower minimum inertia for $k$-Means++ and yet $k$-Means++ improved version achieved lower inertia on average. Moreover, even though these are randomized algorithms, pretty much every time we ran the experiments, $k$-Means++ improved version performed better. Given the mathematical intuitive justification and experimental results, we are inclined to say that this version works better than original $k$-Means++ consistently.
	\begin{thebibliography}{99}
		\bibitem[Arthur et al.(2007)]{kmeans++} D. Arthur and S. Vassilvitskii. \texttt{k-means++}: The Advantages of Careful Seeding. Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms. Society for Industrial and Applied Mathematics Philadelphia, PA, USA. pp. $1027–1035$.
		\bibitem[Forgy(1965)]{forgy} E. Forgy. Cluster analysis of multivariate data: Efficiency versus interpretability of classification. Biometrics, $21, 768-780, 1965$.
		\bibitem[Lloyd(1982)]{lloyd} Stuart P. Lloyd. Least squares quantization in pcm. IEEE Transactions on Information Theory, $28(2):129–136, 1982$.
		\bibitem[Dyer(1985)]{dyer} M. E. Dyer. A simple heuristic for the p-center problem. Operations Research Letters, Volume $3$, February $1985$, pp. $285-288$.
		\bibitem[Miligan et al.(1988)]{miligan} G. Milligan, M. C. Coope. A Study of Standardization of Variables in Cluster Analysis, Journal of Classification 5(2) (1988) 181–204.
		\bibitem[Ostrovsky et al.(2006)]{ostrovsky} R. Ostrovsky, Y. Rabani, Leonard J. Schulman, C. Swamy. The Effectiveness of Lloyd-Type Methods for the k-Means Problem. Proceedings of the $47$th Annual Symposium on Foundations of Computer Science. $2006$.
		\bibitem[Celebi et al.(2013)]{celebi} M. Emre Celebi, Hassan A. Kingravi, Patricio A. Vela. A Comparative Study of Efficient Initialization Methods for the K-Means Clustering Algorithm. Expert Systems with Applications, $40(1): 200–210, 2013$.
		\bibitem[Apostol(1976)]{apostol} Tom M. Apostol. Introduction to Analytic Number Theory. Springer, $10.1007/978-3-662-28579-4$, $1976$.
	\end{thebibliography}
\end{document}