\documentclass[10pt, leqno]{article}

\usepackage{amsthm, amssymb, amsfonts, amsmath, geometry}
\geometry{left=1in,right=1in,top=.5in,bottom=1in,headheight=4pt,headsep=0.3in}

\newcommand{\x}{\mathbf{x}}


\author{Masum Billal}
\title{\bfseries Improving K-Means Algorithm}

\begin{document}
	\maketitle
		\begin{abstract}
			K-Means is a popular clustering algorithm that reduces sum of minimum distances from data points to centers. Lloyd's algorithm uses random centers for initialization. That was improved by Arthur and Vssilvitskii \cite{kmeans++} in the so called \texttt{k-means++} algorithm. In \texttt{k-means++}, instead of choosing initial centers randomly, they are chosen based on a probability that is called \textit{$D^2$ weighting} by the authors. In this paper, we discuss another way of initial center choosing and a possible way of generalizing the procedure. This is not a novel approach towards solving the K-Means problem, rather an investigation into the initialization of centers. Experiments show that our approach can achieve the same accuracy and speed as \texttt{k-means++} algorithm.
		\end{abstract}
	\section{Introduction}
	Widely regarded as the most popular clustering techniques, K-Means remains a humble interesting topic in machine learning as well as computational geometry. Roughly the problem is: given a set of points $\mathbb{X}$ in $\mathbb{R}^d$. Find a set of centers $\mathcal{C}$ such that the function (we already used the name potential function)
		\begin{align*}
			\phi_{\mathcal{C}}(\mathbb{X}) & = \sum_{x\in\mathbb{X}}\min_{c\in\mathcal{C}}(||c-x||^2)
		\end{align*}
	is minimum. We simply want to minimize $\phi_{\mathcal{C}}(\mathbb{X})$ as much as possible. In short, we can denote this using $\phi$ alone when it is clear what $\mathcal{C}$ and $\mathbb{X}$ are.
	
	Default K-Means algorithm starts with random centers and then converge based on minimum distances of the centers from the data points. New centers are calculated based on the centroid. This is known as Lloyd's algorithm. This is done until no more change is possible. K-Means++ takes it one step further by choosing the initial centers carefully. Only the first center is chosen at random. Then the rest of the $k-1$ centers are chosen using \textit{$D^2$ weighting} as Arthur and Vassilvitskii call it\cite{kmeans++}. At first it seems very surprising that no one really wants to work on improving on the centers. However, that is easily explained with the following.
		\begin{align*}
			E & = \sum_{i=1}^n||\x_i-\x||^2\\
			\implies E & = \sum_{i=1}^n(\x_i-\x)I(\x_i-\x)^{T}\\
			\implies \dfrac{\partial{E}}{\partial{x}} & = -2\sum_{i=1}^n(\x_i-\x)\\
			\dfrac{\partial{E}}{\partial{x}} & = 0\\
			\iff \x & = \dfrac{\sum_{i=1}^n\x_i}{n}
		\end{align*}
	where $I$ is the identity matrix of the same rank as $\x$. This pretty much shows why taking the average of all the coordinates in a cluster minimizes the sum of squared distances.
	
	We will first describe K-Means and K-Means++ algorithms. Then we will discuss our ideas and experimental results.
	\begin{thebibliography}{99}
		\bibitem{kmeans++} David Arthur and Sergei Vassilvitskii. \texttt{k-means++}: The Advantages of Careful Seeding. Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms. Society for Industrial and Applied Mathematics Philadelphia, PA, USA. pp. $1027â€“1035$.
	\end{thebibliography}
\end{document}